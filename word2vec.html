<html>
    <body>
        <p>word2vec的理论部分和实现部分我感觉有一些断层。在理解了实现部分之后再去对照它的理论部分的NNLM，也许更容易一些。但实际上它的实现本身就具有合理的数学解释。

<p>word2vec的目标是在嵌入空间里为每个词找一个合适的位置向量。这个向量能够反映词的语法和语义上的一些含义。

<p>word2vec的做法用白话讲就是：对于相邻的词，即出现在同一个采样窗口里，把它们的向量互相拉近一些，拉近的程度用(1-sig(uv))决定。对于不相邻的词，即负采样的词，把他们的向量互相推远一些，推远的程度用sig(uv)决定。这里sig()是sigmoid()函数, 1/(1+exp(-x))

<p>2个向量的内积越大，则这2个向量越趋于同一个方向，且模越大。所以上面说的“拉近”“推远”是指的cosine距离上的意义，而不是欧氏距离的意义。

<p>下面是推导。 

<p>word2vec模型里用log(sig(uv))表示正采样的词与中心词的目标函数，训练的目标是增大这个函数。log(sig(uv))对u求偏导得到的结果恰好是(1-sig(uv))v，对u每次更新的公式即为u = u + (1-sig(uv))v * alpha，alpha是学习率。几何上理解就是把u向v的方向靠近了一些。

<p>这里u和v都是向量，求偏导严格说是对它们的每个分量求，但是并不影响我们叙述的本质。 

<p>对负采样的词，目标函数是log(sig(-uv))，同样是要增大这个值，即让uv变小。log(sig(-uv))对u求偏导的结果是-sig(uv)v，对u的更新公式即为u = u - sig(uv)v * alpha. 几何上理解就是把u向v的方向推远了一些。

<p>在模型里，对每个词是有2套向量的，一套用于中心词，一套对应于窗口词和负采样词。每次更新，我们取一个中心词u，一个窗口词v_o, 若干个负采样词v_i。全部更新公式如下。（向量间是内积，向量和标量间是乘积）

<p>v_o = v_o + (1-sig(v_o*u)) * u * alpha

<p>u = u + (1-sig(v_o*u)) * v_0 * alpha

<p>v_i = v_i - sig(v_i*u) * u * alpha

<p>u = u - sig(v_i*u) * v_i * alpha

<p>注意，在原版的word2vec实现中，u和v的对应关系有点混乱，作者把采样词对应成u，而把中心词和负采样词对应成v，但是由于模型更新公式的对称性，并不会影响结果。

<p>对于Hierarchical Softmax如何理解呢？本质上它是把叶子节点对应到u，把内部节点对应到v。在更新的时候，负采样不像skipgram是随机采样，而是固定在huffman tree路径上的哪些点。我感觉没有明显的好处。
    </body>
</html>